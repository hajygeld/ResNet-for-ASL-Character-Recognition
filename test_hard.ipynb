{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Test Script"
      ],
      "metadata": {
        "id": "9gRkEojx7s-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTMIs62ZgzLU",
        "outputId": "52acdcee-f90f-473a-eaa8-a535edddfcda"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# Custom Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_file, labels_file, transform=None):\n",
        "        self.data = np.load(data_file)\n",
        "        self.labels = np.load(labels_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # image = self.data[idx]\n",
        "        image = self.data[:, idx].reshape(300, 300, 3)  # Reshape the individual image to its original shape\n",
        "        # image = self.data[:, idx].reshape(100, 100, 3)  # Reshape the individual image to its original shape\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "\n",
        "# Filter/kernel dimensions of 3 x 3.\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "# Filter/kernel dimensions of 1 x 1.\n",
        "# 1 x 1 convolutions act as \"bottleneck\" layers, which reduce the number of\n",
        "# parameters to a reasonable size such that computational costs are\n",
        "# decreased.\n",
        "# Number of parameters in a given layer are derived only from the dimensions\n",
        "# of the filter, not from the dimensions of the input image.\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "# According to the paper, each residual block should consist of two\n",
        "# 3 x 3 convolutions with batch normalization and skip connections.\n",
        "# These residual blocks are fed into the ResNet class.\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Main class that takes in a given residual block, the corresponding\n",
        "# number of layers and the number of classes.\n",
        "# Residual blocks are called here, as ResNet consists of a combination\n",
        "# of convolutional, residual and bottleneck layers (blocks) with average\n",
        "# pooling and ReLU as the activation function.\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=1):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        # Fully-connected layer below that has the number of classes\n",
        "        # as the output units.\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.in_planes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.in_planes, planes, stride, downsample))\n",
        "        self.in_planes = planes * block.expansion\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(self.in_planes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def resnet18(num_classes=1):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "\n",
        "\n",
        "def test(test_data_file, test_labels_file):\n",
        "    custom_test_dataset = CustomDataset(test_data_file, test_labels_file)\n",
        "    # Create a DataLoader\n",
        "    batch_size = 64\n",
        "    shuffle = False  # Set to True if you want to shuffle the data\n",
        "\n",
        "    test_loader = DataLoader(dataset=custom_test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Instantiate the model\n",
        "    # model = resnet18(num_classes = 9).to(device)\n",
        "\n",
        "    model_path = 'Best_Model/best_model.pth'\n",
        "\n",
        "    # Load the pre-trained weights\n",
        "    model = torch.load(model_path)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Set the model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        n_correct = 0\n",
        "        n_samples = 0\n",
        "        n_class_correct = [0 for i in range(9)]\n",
        "        n_class_samples = [0 for i in range(9)]\n",
        "\n",
        "        i = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device, dtype=torch.float)\n",
        "            labels = labels.to(device, dtype=torch.long)\n",
        "            images = images.permute(0, 3, 1, 2)  # Permute to [batch_size, 3, 300, 300]\n",
        "            outputs = model(images)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "\n",
        "            # Apply softmax along dim=1\n",
        "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "\n",
        "            # Find the maximum values and their indices along dim=1 for each sample in the batch\n",
        "            max_values, max_indices = torch.max(probabilities, dim=1)\n",
        "            # print(\"indices\", max_indices)\n",
        "            # print(\"probabilities\", max_values)\n",
        "\n",
        "            # Identify indices where values are smaller than 0.5\n",
        "            mask = max_values < 0.5\n",
        "            max_indices[mask] = -1\n",
        "            # print(\"indices\", max_indices)\n",
        "\n",
        "            n_samples += labels.size(0)\n",
        "            n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # consider the identied classes to be -1 as correctly classified\n",
        "            n_correct += (max_indices == -1).sum().item()\n",
        "\n",
        "            for i in range(labels.size(0)):\n",
        "                label = labels[i].item()\n",
        "                pred = predicted[i].item()\n",
        "                if label == pred:\n",
        "                    n_class_correct[label] += 1\n",
        "                n_class_samples[label] += 1\n",
        "\n",
        "        acc = 100.0 * n_correct / n_samples\n",
        "\n",
        "        # class_acc = []\n",
        "        # for i in range(9):\n",
        "        #     class_acc.append(100.0 * n_class_correct[i] / n_class_samples[i])\n",
        "\n",
        "    print(f\"Total Accuracy: {acc}%\")\n",
        "    # for i in range(9):\n",
        "    #     print(f\"Accuracy for class {i}: {class_acc[i]}%\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    test_data_file = 'test_data/test_data.npy'\n",
        "    test_labels_file = 'test_data/test_labels.npy'\n",
        "    # test_data_file = sys.argv[1]\n",
        "    # test_labels_file = sys.argv[2]\n",
        "    test(test_data_file, test_labels_file)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "3QsIT4vVZodX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df273df3-7027-4a63-8792-fdeacff42f2d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Accuracy: 97.22222222222223%\n"
          ]
        }
      ]
    }
  ]
}